# ğŸš€ LLMåŸºç¡€åº”ç”¨å¿«é€Ÿå¼€å§‹

## æ¦‚è¿°

æœ¬ç›®å½•åŒ…å«LLMåŸºç¡€åº”ç”¨çš„ç¤ºä¾‹ä»£ç ï¼Œå¸®åŠ©ä½ å¿«é€Ÿä¸Šæ‰‹ä¸åŒçš„LLM APIè°ƒç”¨ã€‚

## ğŸ“ æ–‡ä»¶ç»“æ„

```
examples/llm-basics/
â”œâ”€â”€ QUICK_START.md          # æœ¬æ–‡ä»¶ - å¿«é€Ÿå¼€å§‹æŒ‡å—
â””â”€â”€ test_llm_client.py      # LLMå®¢æˆ·ç«¯æµ‹è¯•è„šæœ¬
```

## ğŸ¯ å­¦ä¹ ç›®æ ‡

- æŒæ¡ä¸åŒLLM APIçš„åŸºæœ¬è°ƒç”¨æ–¹æ³•
- äº†è§£å‚æ•°è°ƒä¼˜å’Œé…ç½®æŠ€å·§
- å­¦ä¼šå¤„ç†APIå“åº”å’Œé”™è¯¯
- ä¸ºåç»­æ™ºèƒ½ä½“å¼€å‘æ‰“ä¸‹åŸºç¡€

## ğŸ”§ ç¯å¢ƒå‡†å¤‡

### 1. å®‰è£…ä¾èµ–
```bash
pip install -r ../../requirements-minimal.txt
```

### 2. é…ç½®APIå¯†é’¥
è¿è¡Œç¯å¢ƒè®¾ç½®è„šæœ¬ä¼šè‡ªåŠ¨åˆ›å»º `.env` æ–‡ä»¶æ¨¡æ¿ï¼Œç„¶åç¼–è¾‘å¡«å…¥ä½ çš„APIå¯†é’¥ï¼š

```
OPENAI_API_KEY=your_openai_api_key_here
OPENROUTER_API_KEY=your_openrouter_api_key_here
ANTHROPIC_API_KEY=your_anthropic_api_key_here
```

**æ¨èä½¿ç”¨OpenRouter**ï¼šå¯ä»¥åœ¨[openrouter.ai](https://openrouter.ai/)æ³¨å†Œè·å–å…è´¹APIå¯†é’¥

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒè®¾ç½®
```bash
# åœ¨é¡¹ç›®æ ¹ç›®å½•è¿è¡Œ
python setup_env.py
```

### 2. æµ‹è¯•ç¯å¢ƒ
```bash
# åœ¨é¡¹ç›®æ ¹ç›®å½•è¿è¡Œ
python test_env.py
```

### 3. æµ‹è¯•LLMå®¢æˆ·ç«¯
```bash
python test_llm_client.py
```

## ğŸ“š æ ¸å¿ƒæ¦‚å¿µ

### LLMå®¢æˆ·ç«¯æ¶æ„
- `BaseLLMClient`: æŠ½è±¡åŸºç±»ï¼Œå®šä¹‰ç»Ÿä¸€æ¥å£
- `OpenAIClient`: OpenAI APIå®ç°
- `OpenRouterClient`: OpenRouter APIå®ç°
- `AnthropicClient`: Anthropic APIå®ç°
- `LLMClient`: ç»Ÿä¸€ç®¡ç†å™¨ï¼Œæ”¯æŒåŠ¨æ€åˆ‡æ¢

### ä¸»è¦æ–¹æ³•
- `chat_completion()`: èŠå¤©å®Œæˆæ¥å£
- `text_completion()`: æ–‡æœ¬å®Œæˆæ¥å£
- `switch_provider()`: åˆ‡æ¢LLMæä¾›å•†

## ğŸ” ç¤ºä¾‹ä»£ç 

### åŸºç¡€å¯¹è¯
```python
from src.core.llm_client import LLMClient

# åˆ›å»ºå®¢æˆ·ç«¯
client = LLMClient(provider="openai")

# å‘é€æ¶ˆæ¯
messages = [
    {"role": "user", "content": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±"}
]

response = await client.chat_completion(messages)
print(response["content"])
```

### æ–‡æœ¬å®Œæˆ
```python
prompt = "äººå·¥æ™ºèƒ½çš„æœªæ¥æ˜¯"
response = await client.text_completion(prompt, max_tokens=100)
print(f"{prompt}{response}")
```

## ğŸ› ï¸ å‚æ•°è°ƒä¼˜

### å¸¸ç”¨å‚æ•°
- `max_tokens`: æœ€å¤§è¾“å‡ºtokenæ•°
- `temperature`: åˆ›é€ æ€§æ§åˆ¶ (0-2)
- `top_p`: æ ¸é‡‡æ ·å‚æ•° (0-1)
- `frequency_penalty`: é¢‘ç‡æƒ©ç½š
- `presence_penalty`: å­˜åœ¨æƒ©ç½š

### ç¤ºä¾‹
```python
response = await client.chat_completion(
    messages,
    max_tokens=500,
    temperature=0.7,
    top_p=0.9,
    frequency_penalty=0.1
)
```

## ğŸ”§ æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜
1. **APIå¯†é’¥é”™è¯¯**: è¿è¡Œ `python setup_env.py` é‡æ–°è®¾ç½®
2. **ç¯å¢ƒå˜é‡æœªåŠ è½½**: ç¡®ä¿åœ¨é¡¹ç›®æ ¹ç›®å½•è¿è¡Œè„šæœ¬
3. **ç½‘ç»œè¿æ¥**: ç¡®è®¤ç½‘ç»œè¿æ¥æ­£å¸¸
4. **é…é¢é™åˆ¶**: æ£€æŸ¥APIä½¿ç”¨é¢åº¦

### è°ƒè¯•æŠ€å·§
```bash
# æ£€æŸ¥ç¯å¢ƒå˜é‡
python test_env.py

# æŸ¥çœ‹è¯¦ç»†é”™è¯¯ä¿¡æ¯
python -c "from src.core.llm_client import LLMClient; print('å¯¼å…¥æˆåŠŸ')"
```

## ğŸ“– ä¸‹ä¸€æ­¥

å®ŒæˆLLMåŸºç¡€åº”ç”¨å­¦ä¹ åï¼Œå¯ä»¥ç»§ç»­ï¼š

1. **Promptå·¥ç¨‹** â†’ [../prompt-engineering/](../prompt-engineering/) (å¾…å¼€å‘)
2. **æ™ºèƒ½ä½“åº”ç”¨** â†’ [../agents/](../agents/) (å¾…å¼€å‘)
3. **MCPå·¥å…·é›†æˆ** â†’ [../mcp-tools/](../mcp-tools/) (å¾…å¼€å‘)

## ğŸ¯ å­¦ä¹ æˆæœ

å®Œæˆæœ¬æ¨¡å—åï¼Œä½ å°†æŒæ¡ï¼š
- âœ… å¤šç§LLM APIçš„è°ƒç”¨æ–¹æ³•
- âœ… ç¯å¢ƒå˜é‡å’Œé…ç½®ç®¡ç†
- âœ… å‚æ•°è°ƒä¼˜æŠ€å·§
- âœ… é”™è¯¯å¤„ç†å’Œè°ƒè¯•æ–¹æ³•

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤æ”¹è¿›å»ºè®®å’Œä»£ç ç¤ºä¾‹ï¼ 