# ğŸš€ LLMåŸºç¡€åº”ç”¨å¿«é€Ÿå¼€å§‹

## æ¦‚è¿°

æœ¬ç›®å½•åŒ…å«LLMåŸºç¡€åº”ç”¨çš„ç¤ºä¾‹ä»£ç ï¼Œå¸®åŠ©ä½ å¿«é€Ÿä¸Šæ‰‹ä¸åŒçš„LLM APIè°ƒç”¨ã€‚

## ğŸ“ æ–‡ä»¶ç»“æ„

```
examples/llm-basics/
â”œâ”€â”€ QUICK_START.md          # æœ¬æ–‡ä»¶ - å¿«é€Ÿå¼€å§‹æŒ‡å—
â”œâ”€â”€ test_llm_client.py      # LLMå®¢æˆ·ç«¯æµ‹è¯•è„šæœ¬
â”œâ”€â”€ openai_demo.py          # OpenAI APIç¤ºä¾‹
â”œâ”€â”€ openrouter_demo.py      # OpenRouter APIç¤ºä¾‹
â””â”€â”€ anthropic_demo.py       # Anthropic APIç¤ºä¾‹
```

## ğŸ¯ å­¦ä¹ ç›®æ ‡

- æŒæ¡ä¸åŒLLM APIçš„åŸºæœ¬è°ƒç”¨æ–¹æ³•
- äº†è§£å‚æ•°è°ƒä¼˜å’Œé…ç½®æŠ€å·§
- å­¦ä¼šå¤„ç†APIå“åº”å’Œé”™è¯¯
- ä¸ºåç»­æ™ºèƒ½ä½“å¼€å‘æ‰“ä¸‹åŸºç¡€

## ğŸ”§ ç¯å¢ƒå‡†å¤‡

### 1. å®‰è£…ä¾èµ–
```bash
pip install -r ../../requirements-minimal.txt
```

### 2. é…ç½®APIå¯†é’¥
åœ¨é¡¹ç›®æ ¹ç›®å½•åˆ›å»º `.env` æ–‡ä»¶ï¼š
```
OPENAI_API_KEY=your_openai_api_key_here
OPENROUTER_API_KEY=your_openrouter_api_key_here
ANTHROPIC_API_KEY=your_anthropic_api_key_here
```
(å¯ä»¥åœ¨[openrouter](https://openrouter.ai/)æ³¨å†Œkeyä½¿ç”¨)

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. æµ‹è¯•LLMå®¢æˆ·ç«¯
```bash
python test_llm_client.py
```

## ğŸ“š æ ¸å¿ƒæ¦‚å¿µ

### LLMå®¢æˆ·ç«¯æ¶æ„
- `BaseLLMClient`: æŠ½è±¡åŸºç±»ï¼Œå®šä¹‰ç»Ÿä¸€æ¥å£
- `OpenAIClient`: OpenAI APIå®ç°
- `OpenRouterClient`: OpenRouter APIå®ç°
- `AnthropicClient`: Anthropic APIå®ç°
- `LLMClient`: ç»Ÿä¸€ç®¡ç†å™¨ï¼Œæ”¯æŒåŠ¨æ€åˆ‡æ¢

### ä¸»è¦æ–¹æ³•
- `chat_completion()`: èŠå¤©å®Œæˆæ¥å£
- `text_completion()`: æ–‡æœ¬å®Œæˆæ¥å£
- `switch_provider()`: åˆ‡æ¢LLMæä¾›å•†

## ğŸ” ç¤ºä¾‹ä»£ç 

### åŸºç¡€å¯¹è¯
```python
from src.core.llm_client import LLMClient

# åˆ›å»ºå®¢æˆ·ç«¯
client = LLMClient(provider="openai")

# å‘é€æ¶ˆæ¯
messages = [
    {"role": "user", "content": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±"}
]

response = await client.chat_completion(messages)
print(response["content"])
```

### æ–‡æœ¬å®Œæˆ
```python
prompt = "äººå·¥æ™ºèƒ½çš„æœªæ¥æ˜¯"
response = await client.text_completion(prompt, max_tokens=100)
print(f"{prompt}{response}")
```

## ğŸ› ï¸ å‚æ•°è°ƒä¼˜

### å¸¸ç”¨å‚æ•°
- `max_tokens`: æœ€å¤§è¾“å‡ºtokenæ•°
- `temperature`: åˆ›é€ æ€§æ§åˆ¶ (0-2)
- `top_p`: æ ¸é‡‡æ ·å‚æ•° (0-1)
- `frequency_penalty`: é¢‘ç‡æƒ©ç½š
- `presence_penalty`: å­˜åœ¨æƒ©ç½š

### ç¤ºä¾‹
```python
response = await client.chat_completion(
    messages,
    max_tokens=500,
    temperature=0.7,
    top_p=0.9,
    frequency_penalty=0.1
)
```

## ğŸ”§ æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜
1. **APIå¯†é’¥é”™è¯¯**: æ£€æŸ¥ç¯å¢ƒå˜é‡è®¾ç½®
2. **ç½‘ç»œè¿æ¥**: ç¡®è®¤ç½‘ç»œè¿æ¥æ­£å¸¸
3. **é…é¢é™åˆ¶**: æ£€æŸ¥APIä½¿ç”¨é¢åº¦
4. **æ¨¡å‹ä¸å¯ç”¨**: ç¡®è®¤æ¨¡å‹åç§°æ­£ç¡®

### è°ƒè¯•æŠ€å·§
```python
import logging
logging.basicConfig(level=logging.DEBUG)

# å¯ç”¨è¯¦ç»†æ—¥å¿—
client = LLMClient(provider="openai")
```

## ğŸ“– ä¸‹ä¸€æ­¥

å®ŒæˆLLMåŸºç¡€åº”ç”¨å­¦ä¹ åï¼Œå¯ä»¥ç»§ç»­ï¼š

1. **Promptå·¥ç¨‹** â†’ [../prompt-engineering/](../prompt-engineering/)
2. **æ™ºèƒ½ä½“åº”ç”¨** â†’ [../agents/](../agents/)
3. **MCPå·¥å…·é›†æˆ** â†’ [../mcp-tools/](../mcp-tools/)

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤æ”¹è¿›å»ºè®®å’Œä»£ç ç¤ºä¾‹ï¼ 